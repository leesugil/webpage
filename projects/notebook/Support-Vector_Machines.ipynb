{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support-Vector Machines\n",
    "\n",
    "In this mini-series, I'm going to go through the following three classical classification methods\n",
    "- **Support-Vector Classifier (SVC)**\n",
    "- Random Forest Classifier (RFC)\n",
    "- Multilayer Perceptron (MLP)\n",
    "\n",
    "and today is the first of the series, support-vector classifier (also called support vector machine).\n",
    "\n",
    "The main reference is *Fundamentals of Machine Learning* by Thomas P. Trappenberg. I was able to pick up the digital version from the university library. (The version that I got doesn't seem to be the complete version. It's missing some of its elements.) Another great resource is of course Wikipedia.\n",
    "\n",
    "So what is the SVM method in classification problem? We'll see more pictures later, but basically it's one way to draw a line to distinguish who's who (so are the other two classification methods in this series). How one draws the line will be the specific of the method.\n",
    "\n",
    "For this to work, we need a sample data that already tells us who's-who, i.e., a labeled data set. This makes SVM a **supervised learning algorithm** in machine learning. Once the SVM algorithm chooses the best line that seperates the clusters geometrically (this is a non-statistical approach), we use the line to classify new samples even though they are not labeled.\n",
    "\n",
    "Now drawing a line is like a common expression, but in three or higher dimensions, it's really a hyperplane that seperates space. Because we use a hyperplane, it's a linear classifier. There is a version of SVM that uses a hypersurface making non-linear classification possible.\n",
    "\n",
    "There is also a version called the support-vector clustering algorithm, though I wouldn't demonstrate it in this note.\n",
    "\n",
    "We will use Fisher's Iris flower data set which is perhaps everyone's very first example on classification in statistics.\n",
    "\n",
    "Everything that we do here uses the `sklearn` package. The best way to learn about SVM in sklearn is of course to check out the <a href=\"https://scikit-learn.org/stable/modules/svm.html\">official documentation</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.utils.Bunch'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename'])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the Iris flower data using sklearn.\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "print(type(iris))\n",
    "iris.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loaded iris data is the `Bunch` type defined within `sklearn`, and the actual arrays of data can be called using the corresponding keys.\n",
    "\n",
    "We'll mainly use the first two arrays, `iris.data` and `iris.target`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n",
      "(150,)\n"
     ]
    }
   ],
   "source": [
    "print(iris.data.shape)\n",
    "print(iris.target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, they are made out of 150 sample observations. The `iris.data` contains the four features of each sample,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "[5.1 3.5 1.4 0.2]\n"
     ]
    }
   ],
   "source": [
    "print(iris.feature_names)\n",
    "print(iris.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the `iris.target` tells you the actual class of the sample, hence a labeled data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['setosa' 'versicolor' 'virginica']\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(iris.target_names)\n",
    "print(iris.target[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wish to have a model that, given the feature vector $x$ of a sample, the model predicts its class $y$.\n",
    "\n",
    "Like the most of the data-scientific methods, we wouldn't use all 150 samples for training the model, though doing such is likely to yield the best fitting model. The reason is simple: We'd like to check if our model predicts well (validation), so we spare some portion of the data for the use of validation check.\n",
    "\n",
    "Of course it wouldn't do the right validation check if we use the same data for training the model and checking its validation. A trained model is supposed to do its best on the data used in the training, so double-checking on the same data doesn't tell us whether the model is going to perform well on new upcoming samples.\n",
    "\n",
    "How much of a portion should be kept for the validation check? It's a very intellectual question because the model generally works better if more samples were used in training, but we also know the performance of the model better when more samples are used in validation check.\n",
    "\n",
    "Avoiding to answer to the golden-ratio of them, we'll simply use 50-50 in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75, 4)\n",
      "(74, 4)\n",
      "(150, 4)\n"
     ]
    }
   ],
   "source": [
    "# Pick every other samples starting from the 0'th index.\n",
    "x_train = iris.data[0:-1:2]\n",
    "y_train = iris.target[0:-1:2]\n",
    "# Also pick every other samples starting from the 1'st index.\n",
    "x_test = iris.data[1:-1:2]\n",
    "y_test = iris.target[1:-1:2]\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(iris.data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the data sets are ready, time for the application.\n",
    "\n",
    "I wouldn't go through the detailed mathematics here because of the time constraints (though I would go through in detail with a chalk & a board in class). But the main idea and its mathematics can be sufficiently understood if you paid attention during the first month of vector calculus where you learn about the distance between a point and a plane (or a line if it's a 2D example).\n",
    "\n",
    "If you wish to jump to the code (like the future myself), here's a <a href=\"#code\">wormhole</a>.\n",
    "\n",
    "## Idea\n",
    "\n",
    "These 75 training samples with four features are the 75 points in the 4D feature space. We wish to set borderlines (hyperplanes) that help us classify the three distinc clusters. But placing a hyperplane only distinguish two classes at a time (binary classification). Since there are more than two classes to distinguish, we do binary classification multiple times. Thus one might wish to understand the two main ideas:\n",
    "- How does SVM do the binary classification?\n",
    "- How do we apply the binary classification multiple times properly?\n",
    "\n",
    "### Binary Classification in SVM\n",
    "\n",
    "I wrote a seperate page on this with the basic mathematics behind. Check out my another note on <a href=\"The_school_mathematics_behind_SVM.html\">the math behind SVM</a>.\n",
    "\n",
    "The story without math is as the following:\n",
    "\n",
    "If it's possible to place a hyperplane to distinguish the two groups of sample points, SVM places a hyperplane so that the distance from the plane to the closest point of each group is equal and maximized.\n",
    "\n",
    "If it's not possible to place such hyperplane, SVM applies what's called a soft-margin to its criteria and offers the best approximation of the distinction of two groups.\n",
    "\n",
    "### Multi-class Classification in `sklearn.svm.SVC`\n",
    "\n",
    "When there are more than two groups to seperate, there are two classical approaches to apply binary classification multiple times.\n",
    "- One-versus-All (OVA)\n",
    "- One-versus-One (OVO)\n",
    "\n",
    "#### OVA\n",
    "The OVA will compare each group to the rest of the other groups as a whole. In our example, it's going to be like having all of\n",
    "- `setosa` vs `versicolor` + `virginica`,\n",
    "- `versicolor` vs `setosa` + `virginica`,\n",
    "- `virginica` vs `setosa` + `versicolor`.\n",
    "In general, if we have $k$ groups, there will be $k$ binary classifiers obtained from the OVA method.\n",
    "\n",
    "Now given a new sample, we apply all these three classifiers and obtain a score for each. For example, if my iris sample has\n",
    "- `setosa` vs `versicolor` + `virginica` score 0.8,\n",
    "- `versicolor` vs `setosa` + `virginica` score 0.5,\n",
    "- `virginica` vs `setosa` + `versicolor` score -0.1,\n",
    "then the OVA algorithm chooses `setosa` as the predicted class of my sample. This is also called the winner-takes-all strategy in some literatures.\n",
    "\n",
    "The downside of this approach is that each binary classification used in the method has a significant imbalance of sample sizes between two groups of comparison, leading to a possible distorsion of the reality.\n",
    "\n",
    "#### OVO\n",
    "\n",
    "The OVO solves the inherent problem of the OVA approach by comparing groups individually at the cost of performing binary classification more times.\n",
    "\n",
    "If there are $k$ groups in the labeled data, we have to make ${k\\choose2} = \\frac{k (k-1)}{2}$ comparisons. In our example, we have\n",
    "- `setosa` vs `versicolor`,\n",
    "- `setosa` vs `virginica`,\n",
    "- `virginica` vs `versicolor`\n",
    "\n",
    "which happens to require the same amount of binary classifications performed as the OVA case becase we have the magic number $k = 3$ solving\n",
    "$$k = \\frac{k(k-1)}{2}.$$\n",
    "But for $k > 3$, surely OVO is more expensive.\n",
    "\n",
    "In `sklearn.svm.SVC`, the default method is OVO. There is also an option to support the OVA approach, according to the <a hre=\"https://scikit-learn.org/stable/modules/svm.html\">official document</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"code\"></a>\n",
    "## `sklearn.svm.SVC`\n",
    "\n",
    "Here's how to communicate with `sklearn.svm`.\n",
    "- We're going to declare that our model is `svm.SVC`. We have to declare this explicitly because there are other options like `svm.LinearSVC` are available.\n",
    "```python\n",
    "model = svm.SVC(kernel = 'linear')\n",
    "```\n",
    "    - Upon declaring the `svm.SVC` model, declare the kernel which is `linear` in our case. This can be changed if you choose to use non-linear SVM.\n",
    "- All the mathematical algorithms discussed earlier are performed in a single line\n",
    "```python\n",
    "model.fit(x_train, y_train)\n",
    "```\n",
    "(This is why the educators try to convince students that \"*You need to know what you are doing*\" whereas the students keep saying \"*Why do I need to understand the math when the computer does all the computations for me?*\")\n",
    "- Once the model is trained based on the training data, we try to make a prediction of the test data.\n",
    "```python\n",
    "y_predicted = model.predict(x_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage correct accuracy of SVM:  0.972972972972973\n"
     ]
    }
   ],
   "source": [
    "# SVC from sklearn.svm\n",
    "from sklearn import svm\n",
    "\n",
    "# model\n",
    "model = svm.SVC(kernel = 'linear')\n",
    "# train\n",
    "model.fit(x_train, y_train)\n",
    "# prediction\n",
    "y_predicted = model.predict(x_test)\n",
    "# evluation\n",
    "print('Percentage correct accuracy of SVM: ', np.mean(y_test == y_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many types of performance measures and evaluations.\n",
    "- TP: True Positivie\n",
    "- FP: Fals Positive\n",
    "- TN: True Negative\n",
    "- FN: False Negative\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{Accuracy } &= \\frac{TP + TN}{TP + TN + FP + FN} \\\\\n",
    "\\text{True Positive Rate } &= \\frac{TP}{TP + FN} \\\\\n",
    "\\text{False Positive Rate } &= \\frac{FP}{FP + TN} \\\\\n",
    "\\text{Precision } &= \\frac{TP}{TP + FP} \\\\\n",
    "\\text{Recall } &= \\frac{TP}{TP + FN} \\\\\n",
    "F_1 &= 2 \\cdot \\frac{precision \\cdot recall}{precision + recall}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        25\n",
      "           1       0.96      0.96      0.96        25\n",
      "           2       0.96      0.96      0.96        24\n",
      "\n",
      "    accuracy                           0.97        74\n",
      "   macro avg       0.97      0.97      0.97        74\n",
      "weighted avg       0.97      0.97      0.97        74\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# performance measures and evaluations\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "print(metrics.classification_report(y_test, y_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no simple best measure of performance. You have to choose your standard depending on the data.\n",
    "\n",
    "We can also color-code the performance using the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# confusion matrix\n",
    "matrix = metrics.confusion_matrix(y_test, y_predicted)\n",
    "plt.matshow(matrix)\n",
    "plt.colorbar()\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted_label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These measurements can also be simply improved by overfitting, thereby loosing the value of the model.\n",
    "\n",
    "There's another way to improve the validity of the model using the **$k$-fold cross-validation** which we will discuss later <a href=\"#\">later</a>.\n",
    "\n",
    "This page is getting too long, so we'll discuss the **non-linear SVM** in <a href=\"#\">another time</a>."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
