<html>

<head>
	<title>PROJECTS | Sugil Lee</title>
    <link rel="stylesheet" type="text/css" href="../style.css">
    <link rel="icon" type="image/png" href="../img/favicon.png">
	<!-- call top navigator -->
	<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>
	<script>
	$(document).ready(function(){
	$("#nav_top").load("nav1.html");
	});
	</script>
	<!-- call p5 -->
	<script src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/0.10.2/p5.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/0.10.2/addons/p5.sound.min.js"></script>
	<script src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/0.10.2/p5.dom.js"></script>
	<!-- call js -->
	<script src="js/gradient_descent.js"></script>
	<!-- call MathJax -->
	<script type="text/x-mathjax-config">
	MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
	</script>
	<script type="text/javascript"
	src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
	</script>
</head>

<body>
	<div id="nav_top"></div>
	
	<div id="grad1"><h1><br>PROJECTS</h1></div>
	
	<p style="margin: 0; padding: 1% 10% 1% 10%;"><a href="../projects.html">back to projects</a></p>
	<h2>Gradient Descent</h2>
	<p>Hello! This is a sample display of gradient descent for my multivariable calculus class, meaning there is no details on this page yet :(</p>
	<p>I'm hoping to add more on the definition of the gradient of a function and its usage in optimization, data science, and artificial intelligence.</p>
	<p>To give you a slightest idea of what is gradient descent, it is about using the gradient vector of a single-valued function to find the maximum or mininum values (if these ever exist) that are obtained at the critical points of the function. This not only theoretically important but also useful in application because, for example, we often want to know when would a cost function be minimized.</p>
	<div id="sketch-holder" align="center">
	</div>
	<div id="sketch-holder2" align="center">Rotation
	</div>
	<p>For a very, very rough illustration just for the idea, suppose we want to maximize a single-valued function $f(x, y)$. (Some functions, of course, don't have a maximum at all, but we assume $f$ has one.) The goal is to find an input $(a, b)$ such that $f(a, b)$ is bigger than or equal to other values of $f$ evaluated near $(a, b)$, i.e., $(a, b)$ gives a local maximum of $f$. Starting from an arbitarary point $(x_0, y_0)$, for example, the gradient vector $\nabla f$ of $f(x, y)$ at $(x_0, y_0)$ will give us the direction that $(x_0, y_0)$ should move to the next estimation $(x_1, y_1)$ such that the new output $f(x_1, y_1)$ is bigger than the previous output $f(x_0, y_0)$. Repeating this, we expect that $f(x_0, y_0) < f(x_1, y_1) < f(x_2, y_2) < \cdots$ will eventually reach a (locally) maximum value $f(x_n, y_n)$ of the function $f$ at some $(x_n, y_n) = (a, b)$.</p>
	<p>Similarly, applying this to the negative of $f$ will lead us to a minimum value of $f$ (as a max of $-f$), hence gradient <i>descent</i>.</p>
	<p>Gradient descent is a popular algorithm because, in some sense, using gradient is mathematically the right way to achieve extreme values in the general case.</p>
	<p>Even in today's artificial intelligence, we design a learning algorithm that "make machines learn" by giving a key instruction: "learn from the data in a way that you minimize the output of the cost function."</p>
	<p>The green dotted trace shows a sequence of better estimations in $(x, y)$ for a minimum value of $f(x, y)$ using gradient descent of the function given by the surface. The yellow trace on the surface is often more convenient to visualize the gradient descent.</p>
	<p>The source code can be found <a href="js/gradient_descent.js">here</a>.</p>
	<p>Last edited: 2020/5/10</p>
</body>

</html>